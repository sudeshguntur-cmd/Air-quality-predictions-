# -*- coding: utf-8 -*-
"""AIR QUALITY PREDICTION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p1jiJDve7-ql6xisHFz8tdJb--RcHZ8b

BEIJING PM2.5 AIR QUALITY PREDICTION USING DEEP NEURAL NETWORKS

Problem Statement

 Air pollution has become one of the most critical public health challenges in Beijing, China. The city frequently experiences severe air pollution episodes where PM2.5 (particulate matter ≤ 2.5 micrometers) concentrations exceed World Health Organization (WHO) safety guidelines by several times. PM2.5 particles are particularly dangerous because they are small enough to penetrate deep into the lungs and enter the bloodstream, causing:

- Respiratory diseases (asthma, bronchitis, reduced lung function)
- Cardiovascular problems (heart attacks, strokes, irregular heartbeat)
- Premature mortality (estimated 1.6 million deaths annually in China)
- Reduced quality of life (visibility issues, activity restrictions)
- Economic losses (healthcare costs, reduced productivity, tourism impact)

The Beijing Municipal Environmental Monitoring Center operates 12 air quality monitoring stations across the city, collecting hourly measurements of various pollutants and weather conditions. However, current monitoring systems are reactive rather than predictive – they tell us about current pollution levels but cannot forecast future conditions.

 Why This Problem Matters

For Public Health Authorities:
- Need advance warning to issue health alerts to vulnerable populations (elderly, children, people with respiratory conditions)
- Cannot effectively prepare emergency healthcare resources without forecasting capability
- Reactive responses arrive too late to prevent exposure

For Policy Makers:
- Need to implement temporary traffic restrictions or industrial controls during predicted high-pollution periods
- Cannot optimize air quality management without understanding pollution dynamics
- Lack data-driven tools for policy evaluation

For Citizens:
- Want to plan outdoor activities, exercise, and commuting around air quality
- Need to know when to use air purifiers, wear masks, or keep children indoors
- Deserve access to actionable air quality information

Economic Impact:
- Air pollution costs Beijing an estimated $10+ billion annually in healthcare, lost productivity, and reduced quality of life
- Even a 15-20% reduction in pollution exposure could save $2-5 million in healthcare costs
- Tourism and international reputation are negatively affected by pollution episodes

 Current Limitations

Existing approaches have significant weaknesses:

1. No Predictive Capability: Current systems only report current pollution levels, providing no advance warning
2. Simple Statistical Models: Traditional forecasting uses basic time-series methods (ARIMA) that cannot capture complex non-linear relationships
3. Limited Feature Integration: Existing models don't effectively combine pollutant data, weather conditions, and temporal patterns
4. No Actionable Insights: Raw measurements don't translate into clear recommendations for stakeholders

 Proposed Solution

We propose building an **advanced deep learning prediction system using neural networks to forecast PM2.5 concentrations 1+ hours in advance. This system will:

Predict PM2.5 levels with high accuracy (target: RMSE < 30 μg/m³)  

Provide advance warning (1-24 hours ahead) for pollution episodes  

Enable proactive measures (alerts, policy interventions, personal protection)  

Integrate multiple data sources (pollutants, weather, temporal patterns)  

Support decision-making for health officials, policy makers, and citizens

 Data Collection Strategy

Primary Data Source:
- Source: Beijing Municipal Environmental Monitoring Center + China Meteorological Administration
- Station: Aotizhongxin (central Beijing location)
- Time Period: March 1, 2013 to February 28, 2017 (4 years)
- Frequency: Hourly measurements (24 samples per day)
- Total Records: ~35,000 hourly observations

Variables Collected:

Air Quality Measurements:
- PM2.5 (target variable) - Particulate matter ≤ 2.5μm
- PM10 - Particulate matter ≤ 10μm
- SO2 - Sulfur dioxide (from coal burning)
- NO2 - Nitrogen dioxide (from vehicles)
- CO - Carbon monoxide (from combustion)
- O3 - Ozone (secondary pollutant)

Meteorological Data:
- Temperature (°C) - Affects pollutant dispersion
- Pressure (hPa) - Influences atmospheric stability
- Dew Point (°C) - Indicates humidity
- Rainfall (mm) - Washes out pollutants
- Wind Speed (m/s) - Disperses pollutants
- Wind Direction (16 categories) - Determines pollutant transport

Temporal Information:
- Year, Month, Day, Hour - For temporal pattern analysis

Data Quality:
- Collected from nationally-controlled monitoring stations (high quality sensors)
- Weather data matched with nearest meteorological station
- Regular calibration and maintenance protocols
- Some missing values (~2-4% for most variables) due to sensor maintenance or failures
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

np.random.seed(42)
tf.random.set_seed(42)

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.float_format', lambda x: '%.2f' % x)

print("="*80)
print("BEIJING AIR QUALITY PREDICTION PROJECT - AOTIZHONGXIN STATION")
print("="*80)
print(f"TensorFlow version: {tf.__version__}")
print(f"NumPy version: {np.__version__}")
print(f"Pandas version: {pd.__version__}")
print(f"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}")
print("="*80)
print("\n All libraries imported successfully!\n")

"""DATA LOADING AND INITIAL EXPLORATION

Purpose

Before building any model, we must thoroughly understand our data. This section explores:
- Dataset structure and size
- Distribution of PM2.5 (our target variable)
- Missing values and data quality issues
- Relationships between features and PM2.5
- Temporal patterns and trends

What to Look For

1. Data Quality: Are there missing values? Outliers? Sensor errors?
2. PM2.5 Distribution: Is it normally distributed? Skewed? What's the typical range?
3. Correlations: Which features are most related to PM2.5?
4. Temporal Patterns: Do we see daily cycles? Seasonal patterns?

Why This Matters

Understanding the data helps us:
- Choose appropriate preprocessing strategies
- Design relevant features
- Select suitable evaluation metrics
- Set realistic performance expectations
"""

from google.colab import files
uploaded = files.upload()

print(f"Total Records: {len(df):,}")
print(f"Total Features: {len(df.columns)}")
print(f"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
print(f"Station: Aotizhongxin")

print(f"Columns: {list(df.columns)}")

print(df.head())

print(df.info())

print(df.describe())

"""Check for missing values"""

missing = df.isnull().sum()
missing_percent = (missing / len(df)) * 100
missing_df = pd.DataFrame({
    'Column': missing.index,
    'Missing Count': missing.values,
    'Percentage': missing_percent.values
})
missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)
if not missing_df.empty:
    print(missing_df.to_string(index=False))
else:
    print(" No missing values found!")

print(f"Count: {df['PM2.5'].count():,}")
print(f"Mean: {df['PM2.5'].mean():.2f} μg/m³")
print(f"Median: {df['PM2.5'].median():.2f} μg/m³")
print(f"Std Dev: {df['PM2.5'].std():.2f} μg/m³")
print(f"Min: {df['PM2.5'].min():.2f} μg/m³")
print(f"25th percentile: {df['PM2.5'].quantile(0.25):.2f} μg/m³")
print(f"75th percentile: {df['PM2.5'].quantile(0.75):.2f} μg/m³")
print(f"Max: {df['PM2.5'].max():.2f} μg/m³")
print(f"Missing values: {df['PM2.5'].isnull().sum()} ({(df['PM2.5'].isnull().sum()/len(df)*100):.2f}%)")

print(f"   • Good: 0-25 μg/m³")
print(f"   • Moderate: 25-50 μg/m³")
print(f"   • Unhealthy: 50-150 μg/m³")
print(f"   • Very Unhealthy: 150-250 μg/m³")
print(f"   • Hazardous: >250 μg/m³")

fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('Beijing Air Quality - Aotizhongxin Station: Exploratory Analysis',
             fontsize=16, fontweight='bold', y=1.00)

axes[0, 0].hist(df['PM2.5'].dropna(), bins=60, edgecolor='black', alpha=0.7, color='steelblue')
axes[0, 0].axvline(df['PM2.5'].mean(), color='red', linestyle='--', linewidth=2.5,
                   label=f'Mean: {df["PM2.5"].mean():.1f}')
axes[0, 0].axvline(df['PM2.5'].median(), color='green', linestyle='--', linewidth=2.5,
                   label=f'Median: {df["PM2.5"].median():.1f}')
axes[0, 0].set_title('PM2.5 Concentration Distribution', fontsize=13, fontweight='bold')
axes[0, 0].set_xlabel('PM2.5 (μg/m³)', fontsize=11)
axes[0, 0].set_ylabel('Frequency', fontsize=11)
axes[0, 0].legend(fontsize=10)
axes[0, 0].grid(True, alpha=0.3)

sample_size = 2000
sample_data = df.head(sample_size).copy()
sample_data['datetime'] = pd.to_datetime(sample_data[['year', 'month', 'day', 'hour']])
axes[0, 1].plot(sample_data['datetime'], sample_data['PM2.5'], linewidth=1, color='darkgreen', alpha=0.8)
axes[0, 1].set_title(f'PM2.5 Time Series (First {sample_size} hours)', fontsize=13, fontweight='bold')
axes[0, 1].set_xlabel('Date', fontsize=11)
axes[0, 1].set_ylabel('PM2.5 (μg/m³)', fontsize=11)
axes[0, 1].tick_params(axis='x', rotation=45)
axes[0, 1].grid(True, alpha=0.3)

if not missing_df.empty:
    top_missing = missing_df.head(10)
    axes[1, 0].barh(top_missing['Column'], top_missing['Percentage'], color='coral', edgecolor='black')
    axes[1, 0].set_title('Missing Values by Feature (%)', fontsize=13, fontweight='bold')
    axes[1, 0].set_xlabel('Percentage Missing (%)', fontsize=11)
    axes[1, 0].set_ylabel('Feature', fontsize=11)
    axes[1, 0].grid(True, alpha=0.3, axis='x')
else:
    axes[1, 0].text(0.5, 0.5, 'No Missing Values Found!',
                    ha='center', va='center', fontsize=14, fontweight='bold')
    axes[1, 0].set_title('Missing Values Analysis', fontsize=13, fontweight='bold')

numeric_cols = df.select_dtypes(include=[np.number]).columns
correlations = df[numeric_cols].corr()['PM2.5'].sort_values(ascending=False)
correlations = correlations[correlations.index != 'PM2.5']
top_corr = correlations.head(12)
colors = ['green' if x > 0 else 'red' for x in top_corr.values]
axes[1, 1].barh(top_corr.index, top_corr.values, color=colors, edgecolor='black')
axes[1, 1].set_title('Top Features Correlated with PM2.5', fontsize=13, fontweight='bold')
axes[1, 1].set_xlabel('Correlation Coefficient', fontsize=11)
axes[1, 1].set_ylabel('Feature', fontsize=11)
axes[1, 1].axvline(x=0, color='black', linestyle='-', linewidth=1)
axes[1, 1].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

"""DATA CLEANING AND PREPROCESSING

Why Preprocessing is Critical

Raw sensor data cannot be directly fed into neural networks. We need to:
1. Handle Missing Values: Sensors occasionally fail or undergo maintenance
2. Remove Outliers: Extreme sensor errors can distort model learning
3. Create Temporal Features: Neural networks don't understand "time" inherently
4. Normalize Data: Neural networks train better with standardized inputs
5. Engineer Predictive Features: Domain knowledge improves model performance

 Missing Value Strategy

Approach: Forward Fill → Backward Fill

Rationale:
- Air quality changes gradually over hours
- A missing value at 3 PM is likely similar to 2 PM (forward fill)
- If the first measurement is missing, use the next available value (backward fill)
- This preserves temporal continuity better than mean imputation
"""

print(f"\n Creating datetime features...")
df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']])
df = df.sort_values('datetime').reset_index(drop=True)
print(f"DateTime column created and data sorted chronologically")
print(f"   Date range: {df['datetime'].min()} to {df['datetime'].max()}")
print(f"   Duration: {(df['datetime'].max() - df['datetime'].min()).days} days")

original_size = len(df)

pollutant_cols = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3']
weather_cols = ['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']

for col in pollutant_cols + weather_cols:
    if col in df.columns:
        missing_before = df[col].isnull().sum()
        if missing_before > 0:
            df[col] = df[col].fillna(method='ffill').fillna(method='bfill')
            missing_after = df[col].isnull().sum()
            print(f"   {col:8s}: {missing_before:5d} → {missing_after:5d} missing values")
if 'wd' in df.columns and df['wd'].isnull().sum() > 0:
    mode_wd = df['wd'].mode()[0]
    missing_before = df['wd'].isnull().sum()
    df['wd'] = df['wd'].fillna(mode_wd)
    print(f"   {'wd':8s}: {missing_before:5d} → 0 missing values (filled with mode: {mode_wd})")

df_clean = df.dropna()
rows_dropped = original_size - len(df_clean)
print(f"\n Dropped {rows_dropped:,} rows with remaining missing values ({(rows_dropped/original_size*100):.2f}%)")
print(f" Clean dataset size: {len(df_clean):,} rows")

lower_bound = df_clean['PM2.5'].quantile(0.001)
upper_bound = df_clean['PM2.5'].quantile(0.999)
before_outlier = len(df_clean)
df_clean = df_clean[(df_clean['PM2.5'] >= lower_bound) & (df_clean['PM2.5'] <= upper_bound)]
after_outlier = len(df_clean)
outliers_removed = before_outlier - after_outlier

print(f"   PM2.5 valid range: {lower_bound:.2f} - {upper_bound:.2f} μg/m³")
print(f"   Removed {outliers_removed:,} outliers ({(outliers_removed/before_outlier*100):.2f}%)")
print(f" Final clean dataset: {len(df_clean):,} rows")

print(f"   Creating temporal features...")
df_clean['hour'] = df_clean['datetime'].dt.hour
df_clean['day_of_week'] = df_clean['datetime'].dt.dayofweek
df_clean['month'] = df_clean['datetime'].dt.month
df_clean['day_of_year'] = df_clean['datetime'].dt.dayofyear
df_clean['week_of_year'] = df_clean['datetime'].dt.isocalendar().week

df_clean['season'] = df_clean['month'].map({
    12: 4, 1: 4, 2: 4,
    3: 1, 4: 1, 5: 1,
    6: 2, 7: 2, 8: 2,
    9: 3, 10: 3, 11: 3
})

df_clean['is_weekend'] = (df_clean['day_of_week'] >= 5).astype(int)

df_clean['is_rush_hour'] = df_clean['hour'].apply(
    lambda x: 1 if (7 <= x <= 9) or (17 <= x <= 19) else 0
)

print(f"   ✓ Temporal features: hour, day_of_week, month, season, is_weekend, is_rush_hour")

print(f"   Creating cyclical encodings...")
df_clean['hour_sin'] = np.sin(2 * np.pi * df_clean['hour'] / 24)
df_clean['hour_cos'] = np.cos(2 * np.pi * df_clean['hour'] / 24)
df_clean['month_sin'] = np.sin(2 * np.pi * df_clean['month'] / 12)
df_clean['month_cos'] = np.cos(2 * np.pi * df_clean['month'] / 12)
df_clean['day_of_year_sin'] = np.sin(2 * np.pi * df_clean['day_of_year'] / 365)
df_clean['day_of_year_cos'] = np.cos(2 * np.pi * df_clean['day_of_year'] / 365)

print(f"   ✓ Cyclical encodings: hour_sin/cos, month_sin/cos, day_of_year_sin/cos")

print(f"   Encoding wind direction...")
wind_direction_map = {
    'N': 0, 'NNE': 22.5, 'NE': 45, 'ENE': 67.5,
    'E': 90, 'ESE': 112.5, 'SE': 135, 'SSE': 157.5,
    'S': 180, 'SSW': 202.5, 'SW': 225, 'WSW': 247.5,
    'W': 270, 'WNW': 292.5, 'NW': 315, 'NNW': 337.5
}
df_clean['wd_numeric'] = df_clean['wd'].map(wind_direction_map)
df_clean['wd_sin'] = np.sin(2 * np.pi * df_clean['wd_numeric'] / 360)
df_clean['wd_cos'] = np.cos(2 * np.pi * df_clean['wd_numeric'] / 360)

print(f"   ✓ Wind direction: wd_sin, wd_cos")

print(f"   Creating interaction features...")
df_clean['temp_pressure'] = df_clean['TEMP'] * df_clean['PRES']
df_clean['temp_humidity'] = df_clean['TEMP'] * df_clean['DEWP']
df_clean['wind_temp'] = df_clean['WSPM'] * df_clean['TEMP']

print(f"   ✓ Interactions: temp_pressure, temp_humidity, wind_temp")

print(f"   Creating lag features...")
for lag in [1, 2, 3, 6, 12, 24]:
    df_clean[f'PM2.5_lag{lag}'] = df_clean['PM2.5'].shift(lag)

print(f"   ✓ Lag features: PM2.5_lag1, lag2, lag3, lag6, lag12, lag24")

print(f"   Creating rolling window features...")
for window in [3, 6, 12, 24]:
    df_clean[f'PM2.5_rolling_mean_{window}h'] = df_clean['PM2.5'].rolling(window=window, min_periods=1).mean()
    df_clean[f'PM2.5_rolling_std_{window}h'] = df_clean['PM2.5'].rolling(window=window, min_periods=1).std()
    df_clean[f'PM2.5_rolling_max_{window}h'] = df_clean['PM2.5'].rolling(window=window, min_periods=1).max()
    df_clean[f'PM2.5_rolling_min_{window}h'] = df_clean['PM2.5'].rolling(window=window, min_periods=1).min()

print(f"   ✓ Rolling stats: mean, std, max, min for 3h, 6h, 12h, 24h windows")

before_drop = len(df_clean)
df_clean = df_clean.dropna()
after_drop = len(df_clean)
print(f"\n Dropped {before_drop - after_drop:,} rows with NaN from lag/rolling features")

print(f"\n FINAL PROCESSED DATASET:")
print("-"*80)
print(f"   Total features: {len(df_clean.columns)}")
print(f"   Total samples: {len(df_clean):,}")
print(f"   Date range: {df_clean['datetime'].min()} to {df_clean['datetime'].max()}")
print(f"   Missing values: {df_clean.isnull().sum().sum()}")
df_processed = df_clean.copy()

"""TRAIN/VALIDATION/TEST SPLIT

Why Proper Splitting is CRITICAL for Time-Series

The Problem with Random Splitting:

If we randomly shuffle and split, the model would "cheat" by:
- Learning from future data to predict the past
- Seeing patterns that wouldn't exist in real deployment
- Achieving artificially high performance that doesn't generalize

This is called DATA LEAKAGE and invalidates the entire project.

Why This Works:
- Training data comes entirely from the past (2013-2015)
- Validation data is the middle period (2016)
- Test data is the most recent unseen future (2016-2017)
- Mimics real-world deployment: Model trained on historical data, predicts future

 Split Ratios: 70% / 15% / 15%

Training Set (70%):
- Purpose: Learn patterns, adjust weights
- Size: Largest portion to capture diverse pollution scenarios
- Why 70%? Balance between having enough learning examples and reserving data for evaluation

Validation Set (15%):
- Purpose: Hyperparameter tuning, model selection
- Why Separate? If we tuned on test set, we'd overfit to test data
- Role: Acts as a "mini test set" during development

Test Set (15%):
- Purpose: Final unbiased performance estimate
- Critical: NEVER used during training or tuning
- Represents: Real-world future data model will see in deployment
"""

target = 'PM2.5'

exclude_cols = ['No', 'year', 'month', 'day', 'hour', 'datetime', 'wd', 'station',
                'PM2.5', 'wd_numeric', 'day_of_week', 'day_of_year', 'week_of_year']

feature_columns = [col for col in df_processed.columns if col not in exclude_cols]
print(f"   Total features available: {len(df_processed.columns)}")
print(f"   Features selected for modeling: {len(feature_columns)}")
print(f"   Target variable: {target}")

feature_types = {
    'Pollutants': [col for col in feature_columns if col in ['PM10', 'SO2', 'NO2', 'CO', 'O3']],
    'Weather': [col for col in feature_columns if col in ['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']],
    'Temporal': [col for col in feature_columns if any(x in col for x in ['hour', 'month', 'season', 'weekend', 'rush'])],
    'Wind': [col for col in feature_columns if 'wd_' in col],
    'Interactions': [col for col in feature_columns if any(x in col for x in ['temp_pressure', 'temp_humidity', 'wind_temp'])],
    'Lag': [col for col in feature_columns if 'lag' in col],
    'Rolling': [col for col in feature_columns if 'rolling' in col]
}

print(f"\n   Feature breakdown by category:")
for cat, feats in feature_types.items():
    if feats:
        print(f"      {cat:15s}: {len(feats):2d} features")
print(f"   Method: Chronological split (NO random splitting)")
print(f"   Rationale: Prevents data leakage in time-series problems")

train_ratio = 0.70
val_ratio = 0.15
test_ratio = 0.15

train_size = int(train_ratio * len(df_processed))
val_size = int(val_ratio * len(df_processed))

train_df = df_processed.iloc[:train_size].copy()
val_df = df_processed.iloc[train_size:train_size + val_size].copy()
test_df = df_processed.iloc[train_size + val_size:].copy()

print(f"      Samples: {len(train_df):,} ({train_ratio*100:.0f}%)")
print(f"      Date range: {train_df['datetime'].min()} to {train_df['datetime'].max()}")
print(f"      PM2.5 range: {train_df['PM2.5'].min():.1f} - {train_df['PM2.5'].max():.1f} μg/m³")
print(f"   Validation Set:")
print(f"      Samples: {len(val_df):,} ({val_ratio*100:.0f}%)")
print(f"      Date range: {val_df['datetime'].min()} to {val_df['datetime'].max()}")
print(f"      PM2.5 range: {val_df['PM2.5'].min():.1f} - {val_df['PM2.5'].max():.1f} μg/m³")
print(f"   Test Set:")
print(f"      Samples: {len(test_df):,} ({test_ratio*100:.0f}%)")
print(f"      Date range: {test_df['datetime'].min()} to {test_df['datetime'].max()}")
print(f"      PM2.5 range: {test_df['PM2.5'].min():.1f} - {test_df['PM2.5'].max():.1f} μg/m³")

X_train = train_df[feature_columns].values
y_train = train_df[target].values

X_val = val_df[feature_columns].values
y_val = val_df[target].values

X_test = test_df[feature_columns].values
y_test = test_df[target].values

print(f"\n Feature matrices extracted:")
print(f"   X_train shape: {X_train.shape}")
print(f"   X_val shape:   {X_val.shape}")
print(f"   X_test shape:  {X_test.shape}")

print(f"   Method: StandardScaler (mean=0, std=1)")
print(f"   Fit on: Training data only (prevents data leakage)")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

target_scaler = StandardScaler()
y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()
y_val_scaled = target_scaler.transform(y_val.reshape(-1, 1)).flatten()
y_test_scaled = target_scaler.transform(y_test.reshape(-1, 1)).flatten()

print(f"Scaling applied to features and target")

print(f"   Training data:")
print(f"      X_train range: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]")
print(f"      y_train range: [{y_train.min():.1f}, {y_train.max():.1f}] μg/m³")
print(f"   NaN values: {np.isnan(X_train_scaled).sum()} (should be 0)")
print(f"   Infinite values: {np.isinf(X_train_scaled).sum()} (should be 0)")

assert not np.isnan(X_train_scaled).any(), " NaN values detected in training data!"
assert not np.isinf(X_train_scaled).any(), " Infinite values detected in training data!"

print(f"\n Data quality verified - ready for model training!")

INPUT_DIM = X_train_scaled.shape[1]
print(f"\n Input dimension: {INPUT_DIM} features")

"""BUILD AND TRAIN NEURAL NETWORK MODEL"""

print("\n" + "="*80)
print("STEP 4: NEURAL NETWORK MODEL BUILDING AND TRAINING")
print("="*80)

def build_model(input_dim, hidden_layers=[128, 64, 32], dropout_rate=0.3, learning_rate=0.001):
    """
    Build a feedforward neural network for PM2.5 prediction

    Architecture:
        - Input layer with normalization
        - Multiple hidden layers with ReLU activation
        - Batch normalization after each layer
        - Dropout for regularization
        - Linear output layer for regression

    Args:
        input_dim: Number of input features
        hidden_layers: List of neurons in each hidden layer
        dropout_rate: Dropout rate for regularization (0-1)
        learning_rate: Learning rate for Adam optimizer

    Returns:
        Compiled Keras model
    """
    model = Sequential(name='PM25_Predictor')

    model.add(Dense(hidden_layers[0], input_dim=input_dim, activation='relu',
                    kernel_initializer='he_normal', name='first_dense_layer'))
    model.add(BatchNormalization(name='bn_input'))
    model.add(Dropout(dropout_rate, name='dropout_input'))

    for i, units in enumerate(hidden_layers[1:], start=2):
        model.add(Dense(units, activation='relu', kernel_initializer='he_normal',
                       name=f'hidden_{i}'))
        model.add(BatchNormalization(name=f'bn_{i}'))
        model.add(Dropout(dropout_rate, name=f'dropout_{i}'))

    model.add(Dense(1, activation='linear', name='output_layer'))

    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss='mean_squared_error',
        metrics=['mae', 'mse']
    )

    return model

HIDDEN_LAYERS = [128, 64, 32]
DROPOUT_RATE = 0.3
LEARNING_RATE = 0.001
BATCH_SIZE = 128
EPOCHS = 100

print(f"   Architecture: {HIDDEN_LAYERS}")
print(f"   Dropout rate: {DROPOUT_RATE}")
print(f"   Learning rate: {LEARNING_RATE}")
print(f"   Batch size: {BATCH_SIZE}")
print(f"   Max epochs: {EPOCHS}")
print(f"   Input features: {INPUT_DIM}")
print(f"   Output: 1 (PM2.5 concentration)")

model = build_model(
    input_dim=INPUT_DIM,
    hidden_layers=HIDDEN_LAYERS,
    dropout_rate=DROPOUT_RATE,
    learning_rate=LEARNING_RATE
)

print(f"\n MODEL ARCHITECTURE SUMMARY:")
print("="*80)
model.summary()

total_params = model.count_params()
print(f"\n Model built successfully!")
print(f"   Total parameters: {total_params:,}")

print(f"\n TRAINING CALLBACKS:")
print("-"*80)

early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=15,
    restore_best_weights=True,
    verbose=1,
    mode='min'
)
print(f"   ✓ Early Stopping: patience=15 epochs")

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-7,
    verbose=1,
    mode='min'
)
print(f"   ✓ Learning Rate Reduction: factor=0.5, patience=5")

model_checkpoint = ModelCheckpoint(
    'best_pm25_model.keras',
    monitor='val_loss',
    save_best_only=True,
    verbose=0,
    mode='min'
)
print(f"   ✓ Model Checkpoint: saves best model to 'best_pm25_model.keras'")

callbacks_list = [early_stopping, reduce_lr, model_checkpoint]

print(f"\n STARTING MODEL TRAINING...")
print("="*80)
print(f"Training on {len(X_train_scaled):,} samples")
print(f"Validating on {len(X_val_scaled):,} samples")
print(f"This may take several minutes...\n")

history = model.fit(
    X_train_scaled, y_train_scaled,
    validation_data=(X_val_scaled, y_val_scaled),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=callbacks_list,
    verbose=1
)

print(f"\n TRAINING COMPLETE!")
print(f"   Epochs trained: {len(history.history['loss'])}")
print(f"   Final training loss: {history.history['loss'][-1]:.4f}")
print(f"   Final validation loss: {history.history['val_loss'][-1]:.4f}")

print(f"\n Generating training history plots...")

fig, axes = plt.subplots(1, 2, figsize=(16, 5))
fig.suptitle('Neural Network Training History - Aotizhongxin Station',
             fontsize=14, fontweight='bold')

axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2, color='blue')
axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2, color='red')
axes[0].set_title('Model Loss (MSE) Over Epochs', fontsize=12, fontweight='bold')
axes[0].set_xlabel('Epoch', fontsize=11)
axes[0].set_ylabel('Loss (MSE)', fontsize=11)
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)

axes[1].plot(history.history['mae'], label='Training MAE', linewidth=2, color='blue')
axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2, color='red')
axes[1].set_title('Mean Absolute Error Over Epochs', fontsize=12, fontweight='bold')
axes[1].set_xlabel('Epoch', fontsize=11)
axes[1].set_ylabel('MAE', fontsize=11)
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\n VALIDATION SET EVALUATION:")
print("="*80)

y_val_pred_scaled = model.predict(X_val_scaled, verbose=0)
y_val_pred = target_scaler.inverse_transform(y_val_pred_scaled).flatten()

val_mse = mean_squared_error(y_val, y_val_pred)
val_rmse = np.sqrt(val_mse)
val_mae = mean_absolute_error(y_val, y_val_pred)
val_r2 = r2_score(y_val, y_val_pred)
val_mape = np.mean(np.abs((y_val - y_val_pred) / (y_val + 1e-8))) * 100

print(f"\n   Performance Metrics:")
print(f"   ├── RMSE:  {val_rmse:.2f} μg/m³")
print(f"   ├── MAE:   {val_mae:.2f} μg/m³")
print(f"   ├── MAPE:  {val_mape:.2f}%")
print(f"   └── R²:    {val_r2:.4f}")

print(f"\n   Interpretation:")
print(f"   • Predictions are off by {val_mae:.1f} μg/m³ on average")
print(f"   • Model explains {val_r2*100:.1f}% of PM2.5 variance")
print(f"   • Typical error is {val_mape:.1f}% of actual value")

print(f"\n Generating validation prediction visualizations...")

fig, axes = plt.subplots(1, 2, figsize=(16, 6))
fig.suptitle('Validation Set Predictions - Aotizhongxin Station',
             fontsize=14, fontweight='bold')

axes[0].scatter(y_val, y_val_pred, alpha=0.4, s=15, color='blue', edgecolors='navy', linewidth=0.3)
axes[0].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()],
             'r--', linewidth=2.5, label='Perfect Prediction', alpha=0.8)
axes[0].set_xlabel('Actual PM2.5 (μg/m³)', fontsize=11)
axes[0].set_ylabel('Predicted PM2.5 (μg/m³)', fontsize=11)
axes[0].set_title(f'Actual vs Predicted (R²={val_r2:.4f})', fontsize=12, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)

residuals = y_val - y_val_pred
axes[1].scatter(y_val_pred, residuals, alpha=0.4, s=15, color='purple', edgecolors='darkviolet', linewidth=0.3)
axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2.5, alpha=0.8)
axes[1].set_xlabel('Predicted PM2.5 (μg/m³)', fontsize=11)
axes[1].set_ylabel('Residuals (Actual - Predicted)', fontsize=11)
axes[1].set_title('Residual Plot', fontsize=12, fontweight='bold')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\n Baseline model training and validation complete!")

"""TESTING MULTIPLE CONFIGURATIONS"""

print("Testing 12 different configurations to find optimal architecture...\n")

experiments = [
    {'name': 'Baseline (3 layers)', 'hidden_layers': [128, 64, 32], 'dropout_rate': 0.3, 'learning_rate': 0.001, 'batch_size': 128},
    {'name': 'Deeper (4 layers)', 'hidden_layers': [256, 128, 64, 32], 'dropout_rate': 0.3, 'learning_rate': 0.001, 'batch_size': 128},
    {'name': 'Wider Network', 'hidden_layers': [256, 256, 128], 'dropout_rate': 0.3, 'learning_rate': 0.001, 'batch_size': 128},
    {'name': 'Shallow (2 layers)', 'hidden_layers': [64, 32], 'dropout_rate': 0.3, 'learning_rate': 0.001, 'batch_size': 128},
    {'name': 'High Dropout (0.5)', 'hidden_layers': [128, 64, 32], 'dropout_rate': 0.5, 'learning_rate': 0.001, 'batch_size': 128},
    {'name': 'Low Dropout (0.1)', 'hidden_layers': [128, 64, 32], 'dropout_rate': 0.1, 'learning_rate': 0.001, 'batch_size': 128},
    {'name': 'High LR (0.01)', 'hidden_layers': [128, 64, 32], 'dropout_rate': 0.3, 'learning_rate': 0.01, 'batch_size': 128},
    {'name': 'Low LR (0.0001)', 'hidden_layers': [128, 64, 32], 'dropout_rate': 0.3, 'learning_rate': 0.0001, 'batch_size': 128},
    {'name': 'Large Batch (256)', 'hidden_layers': [128, 64, 32], 'dropout_rate': 0.3, 'learning_rate': 0.001, 'batch_size': 256},
    {'name': 'Small Batch (64)', 'hidden_layers': [128, 64, 32], 'dropout_rate': 0.3, 'learning_rate': 0.001, 'batch_size': 64},
    {'name': 'Complex (5 layers)', 'hidden_layers': [512, 256, 128, 64, 32], 'dropout_rate': 0.4, 'learning_rate': 0.0005, 'batch_size': 128},
    {'name': 'Simple (2 small)', 'hidden_layers': [32, 16], 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 128}
]

results = []
best_val_loss = float('inf')
best_model_exp = None

for i, exp in enumerate(experiments, 1):
    print(f"{'='*80}")
    print(f"Experiment {i}/{len(experiments)}: {exp['name']}")
    print(f"{'='*80}")
    print(f"Config: layers={exp['hidden_layers']}, dropout={exp['dropout_rate']}, "
          f"lr={exp['learning_rate']}, batch={exp['batch_size']}")

    model_exp = build_model(
        input_dim=INPUT_DIM,
        hidden_layers=exp['hidden_layers'],
        dropout_rate=exp['dropout_rate'],
        learning_rate=exp['learning_rate']
    )

    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0)
    reduce_lr_exp = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=0)

    history_exp = model_exp.fit(
        X_train_scaled, y_train_scaled,
        validation_data=(X_val_scaled, y_val_scaled),
        epochs=50,
        batch_size=exp['batch_size'],
        callbacks=[early_stop, reduce_lr_exp],
        verbose=0
    )

    y_val_pred_scaled_exp = model_exp.predict(X_val_scaled, verbose=0)
    y_val_pred_exp = target_scaler.inverse_transform(y_val_pred_scaled_exp).flatten()

    rmse = np.sqrt(mean_squared_error(y_val, y_val_pred_exp))
    mae = mean_absolute_error(y_val, y_val_pred_exp)
    r2 = r2_score(y_val, y_val_pred_exp)
    mape = np.mean(np.abs((y_val - y_val_pred_exp) / (y_val + 1e-8))) * 100
    epochs_trained = len(history_exp.history['loss'])
    final_val_loss = history_exp.history['val_loss'][-1]

    if final_val_loss < best_val_loss:
        best_val_loss = final_val_loss
        best_model_exp = model_exp

    results.append({
        'Experiment': exp['name'],
        'Architecture': str(exp['hidden_layers']),
        'Dropout': exp['dropout_rate'],
        'LR': exp['learning_rate'],
        'Batch': exp['batch_size'],
        'RMSE': rmse,
        'MAE': mae,
        'R²': r2,
        'MAPE': mape,
        'Epochs': epochs_trained
    })

    print(f" Results: RMSE={rmse:.2f}, MAE={mae:.2f}, R²={r2:.4f}, Epochs={epochs_trained}\n")
results_df = pd.DataFrame(results)
results_df = results_df.sort_values('RMSE', ascending=True).reset_index(drop=True)
results_df['Rank'] = range(1, len(results_df) + 1)

print(f"\n{'='*80}")
print("EXPERIMENT RESULTS SUMMARY (Sorted by RMSE)")
print(f"{'='*80}\n")
print(results_df[['Rank', 'Experiment', 'RMSE', 'MAE', 'R²', 'MAPE']].to_string(index=False))

best_exp = results_df.iloc[0]
print(f"\n{'='*80}")
print(" BEST PERFORMING MODEL")
print(f"{'='*80}")
print(f"Name: {best_exp['Experiment']}")
print(f"Architecture: {best_exp['Architecture']}")
print(f"Dropout: {best_exp['Dropout']}")
print(f"Learning Rate: {best_exp['LR']}")
print(f"Batch Size: {best_exp['Batch']}")
print(f"\nPerformance:")
print(f"   ├── RMSE: {best_exp['RMSE']:.2f} μg/m³")
print(f"   ├── MAE:  {best_exp['MAE']:.2f} μg/m³")
print(f"   ├── R²:   {best_exp['R²']:.4f}")
print(f"   └── MAPE: {best_exp['MAPE']:.2f}%")

print(f"\n Generating experiment comparison visualizations...")

fig, axes = plt.subplots(2, 2, figsize=(18, 12))
fig.suptitle('Model Configuration Experiments - Performance Comparison',
             fontsize=15, fontweight='bold')

axes[0, 0].barh(results_df['Experiment'], results_df['RMSE'], color='steelblue', edgecolor='navy')
axes[0, 0].set_xlabel('RMSE (μg/m³)', fontsize=11)
axes[0, 0].set_title('RMSE Comparison Across Experiments', fontsize=12, fontweight='bold')
axes[0, 0].invert_yaxis()
axes[0, 0].grid(axis='x', alpha=0.3)
axes[0, 0].axvline(x=best_exp['RMSE'], color='red', linestyle='--', linewidth=2, alpha=0.7, label='Best')
axes[0, 0].legend()

axes[0, 1].barh(results_df['Experiment'], results_df['MAE'], color='coral', edgecolor='darkred')
axes[0, 1].set_xlabel('MAE (μg/m³)', fontsize=11)
axes[0, 1].set_title('MAE Comparison Across Experiments', fontsize=12, fontweight='bold')
axes[0, 1].invert_yaxis()
axes[0, 1].grid(axis='x', alpha=0.3)
axes[0, 1].axvline(x=best_exp['MAE'], color='red', linestyle='--', linewidth=2, alpha=0.7, label='Best')
axes[0, 1].legend()

axes[1, 0].barh(results_df['Experiment'], results_df['R²'], color='green', edgecolor='darkgreen')
axes[1, 0].set_xlabel('R² Score', fontsize=11)
axes[1, 0].set_title('R² Score Comparison Across Experiments', fontsize=12, fontweight='bold')
axes[1, 0].invert_yaxis()
axes[1, 0].grid(axis='x', alpha=0.3)
axes[1, 0].axvline(x=best_exp['R²'], color='red', linestyle='--', linewidth=2, alpha=0.7, label='Best')
axes[1, 0].legend()

axes[1, 1].barh(results_df['Experiment'], results_df['MAPE'], color='purple', edgecolor='indigo')
axes[1, 1].set_xlabel('MAPE (%)', fontsize=11)
axes[1, 1].set_title('MAPE Comparison Across Experiments', fontsize=12, fontweight='bold')
axes[1, 1].invert_yaxis()
axes[1, 1].grid(axis='x', alpha=0.3)
axes[1, 1].axvline(x=best_exp['MAPE'], color='red', linestyle='--', linewidth=2, alpha=0.7, label='Best')
axes[1, 1].legend()

plt.tight_layout()
plt.show()

print(f"\n All experiments completed!")

"""FINAL MODEL EVALUATION ON TEST SET

We systematically vary ONE aspect at a time to isolate its effect:

Variables to Test:
1. Architecture Depth: 2 layers vs 3 vs 4 vs 5
2. Architecture Width: Narrow [64,32] vs Wide [256,256,128]
3. Dropout Rate: Low (0.1) vs Medium (0.3) vs High (0.5)
4. Learning Rate: Low (0.0001) vs Medium (0.001) vs High (0.01)
5. Batch Size: Small (64) vs Medium (128) vs Large (256)

 Experiment Hypotheses

 Experiment 1: Deeper Network [256, 128, 64, 32]
Hypothesis: More layers → Better feature learning → Higher accuracy  
Expected: Marginal improvement OR overfitting (too complex)

 Experiment 2: Wider Network [256, 256, 128]
Hypothesis: More neurons → More capacity → Better patterns  
Expected: Similar performance, longer training time

Experiment 3: Shallow Network [64, 32]
Hypothesis: Simpler model → Faster training but lower accuracy  
Expected: Underfit (can't capture complexity)

Experiment 4: High Dropout (0.5)
Hypothesis: More regularization → Prevents overfitting  
Expected: Lower training accuracy, possibly better validation

Experiment 5: Low Dropout (0.1)
Hypothesis: Less regularization → Better training, risk overfitting  
Expected: Higher training accuracy, possibly worse validation

Experiment 6: High Learning Rate (0.01)
Hypothesis: Faster convergence but risk of instability  
Expected: Fast initial learning, possible oscillation

 Experiment 7: Low Learning Rate (0.0001)
Hypothesis: Slower but more stable convergence  
Expected: Slower training, potentially better final performance

 Experiment 8: Large Batch (256)
Hypothesis: Smoother gradients, more stable training  
Expected: Slower epochs, possibly better generalization

 Experiment 9: Small Batch (64)
**Hypothesis:** Noisier gradients, faster epochs  
**Expected:** More updates per epoch, possibly faster convergence

 Experiment 10-12: Complex/Simple Extremes
Test boundaries: What happens at extremes of complexity?

 Evaluation Metrics

For each experiment, we track:
- RMSE (Root Mean Squared Error): Primary metric - penalizes large errors
- MAE (Mean Absolute Error): Average prediction error in μg/m³
- R² Score: Proportion of variance explained (0 to 1, higher better)
- MAPE (Mean Absolute Percentage Error): Relative error
- Epochs Trained: How long until convergence?

 How to Interpret Results

Best Model Indicators:
-  Lowest RMSE (most accurate overall)
-  Lowest MAE (smallest average error)
-  Highest R² (best variance explanation)
-  No signs of overfitting (train/val gap small)
"""

print(f"\nUsing baseline model for final test evaluation...")
print(f"(In practice, you would retrain the best configuration from experiments)")

print(f"\nMaking predictions on {len(X_test_scaled):,} test samples...")
y_test_pred_scaled = model.predict(X_test_scaled, verbose=0)
y_test_pred = target_scaler.inverse_transform(y_test_pred_scaled).flatten()

test_mse = mean_squared_error(y_test, y_test_pred)
test_rmse = np.sqrt(test_mse)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)
test_mape = np.mean(np.abs((y_test - y_test_pred) / (y_test + 1e-8))) * 100

test_max_error = np.max(np.abs(y_test - y_test_pred))
test_median_ae = np.median(np.abs(y_test - y_test_pred))

print(f"\n FINAL TEST SET PERFORMANCE METRICS:")
print(f"{'='*80}")
print(f"   Root Mean Squared Error (RMSE): {test_rmse:.2f} μg/m³")
print(f"   Mean Absolute Error (MAE):      {test_mae:.2f} μg/m³")
print(f"   Median Absolute Error:          {test_median_ae:.2f} μg/m³")
print(f"   Mean Absolute % Error (MAPE):   {test_mape:.2f}%")
print(f"   R² Score:                        {test_r2:.4f}")
print(f"   Maximum Error:                   {test_max_error:.2f} μg/m³")
print(f"{'='*80}")

print(f"\n INTERPRETATION:")
print(f"   • Predictions are typically off by {test_mae:.1f} μg/m³")
print(f"   • Model explains {test_r2*100:.1f}% of PM2.5 variance")
print(f"   • Average percentage error is {test_mape:.1f}%")
print(f"   • Worst case error: {test_max_error:.1f} μg/m³")

print(f"\n PERFORMANCE BY POLLUTION LEVEL:")
print("-"*80)

def categorize_pm25(value):
    if value < 25: return 'Good (0-25)'
    elif value < 50: return 'Moderate (25-50)'
    elif value < 150: return 'Unhealthy (50-150)'
    elif value < 250: return 'Very Unhealthy (150-250)'
    else: return 'Hazardous (>250)'

test_df_eval = pd.DataFrame({
    'actual': y_test,
    'predicted': y_test_pred,
    'category': [categorize_pm25(val) for val in y_test]
})

for category in ['Good (0-25)', 'Moderate (25-50)', 'Unhealthy (50-150)', 'Very Unhealthy (150-250)', 'Hazardous (>250)']:
    cat_data = test_df_eval[test_df_eval['category'] == category]
    if len(cat_data) > 0:
        cat_mae = mean_absolute_error(cat_data['actual'], cat_data['predicted'])
        cat_r2 = r2_score(cat_data['actual'], cat_data['predicted'])
        print(f"   {category:25s}: n={len(cat_data):5d}, MAE={cat_mae:6.2f}, R²={cat_r2:6.4f}")

print(f"\n Generating comprehensive test set visualizations...")

fig = plt.figure(figsize=(18, 12))
gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
fig.suptitle('Final Model Evaluation - Test Set Performance (Aotizhongxin Station)',
             fontsize=15, fontweight='bold')

ax1 = fig.add_subplot(gs[0:2, 0:2])
ax1.scatter(y_test, y_test_pred, alpha=0.4, s=20, color='blue', edgecolors='navy', linewidth=0.3)
ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
         'r--', linewidth=3, label='Perfect Prediction', alpha=0.8)
ax1.set_xlabel('Actual PM2.5 (μg/m³)', fontsize=12, fontweight='bold')
ax1.set_ylabel('Predicted PM2.5 (μg/m³)', fontsize=12, fontweight='bold')
ax1.set_title(f'Actual vs Predicted (R²={test_r2:.4f}, RMSE={test_rmse:.2f})',
             fontsize=13, fontweight='bold')
ax1.legend(fontsize=11)
ax1.grid(True, alpha=0.3)

ax2 = fig.add_subplot(gs[0, 2])
residuals_test = y_test - y_test_pred
ax2.scatter(y_test_pred, residuals_test, alpha=0.4, s=15, color='purple', edgecolors='darkviolet', linewidth=0.3)
ax2.axhline(y=0, color='r', linestyle='--', linewidth=2.5, alpha=0.8)
ax2.set_xlabel('Predicted PM2.5 (μg/m³)', fontsize=10)
ax2.set_ylabel('Residuals', fontsize=10)
ax2.set_title('Residual Plot', fontsize=11, fontweight='bold')
ax2.grid(True, alpha=0.3)

ax3 = fig.add_subplot(gs[1, 2])
ax3.hist(residuals_test, bins=50, edgecolor='black', alpha=0.7, color='orange')
ax3.axvline(x=0, color='r', linestyle='--', linewidth=2.5)
ax3.set_xlabel('Prediction Error (μg/m³)', fontsize=10)
ax3.set_ylabel('Frequency', fontsize=10)
ax3.set_title('Error Distribution', fontsize=11, fontweight='bold')
ax3.grid(True, alpha=0.3, axis='y')

ax4 = fig.add_subplot(gs[2, :])
sample_size = min(1000, len(y_test))
time_index = range(sample_size)
ax4.plot(time_index, y_test[:sample_size], label='Actual', linewidth=1.5, alpha=0.8, color='green')
ax4.plot(time_index, y_test_pred[:sample_size], label='Predicted', linewidth=1.5, alpha=0.8, color='red')
ax4.set_xlabel('Time Steps', fontsize=11, fontweight='bold')
ax4.set_ylabel('PM2.5 (μg/m³)', fontsize=11, fontweight='bold')
ax4.set_title(f'Actual vs Predicted Over Time (First {sample_size} test samples)',
             fontsize=12, fontweight='bold')
ax4.legend(fontsize=11)
ax4.grid(True, alpha=0.3)

plt.show()

"""FEATURE IMPORTANCE AND BUSINESS RECOMMENDATIONS"""

print(f"\n FEATURE SUMMARY:")
feature_info_df = pd.DataFrame({
    'Feature': feature_columns,
    'Type': ['Pollutant' if col in ['PM10', 'SO2', 'NO2', 'CO', 'O3']
             else 'Weather' if col in ['TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']
             else 'Temporal' if any(x in col for x in ['hour', 'month', 'season', 'weekend', 'rush', 'day'])
             else 'Wind' if 'wd_' in col
             else 'Interaction' if any(x in col for x in ['temp_', 'wind_'])
             else 'Lag' if 'lag' in col
             else 'Rolling' if 'rolling' in col
             else 'Other'
             for col in feature_columns]
})

feature_summary = feature_info_df['Type'].value_counts().sort_index()
print(feature_summary)